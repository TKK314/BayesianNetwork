[["index.html", "1 本サイトについて", " 1 本サイトについて Rと事例で学ぶベイジアンネットワークをもとに勉強をすすめています bnlearn - Bayesian network structure learningに参考となるdataファイルやRコードが記載されています "],["離散型データ事例.html", "2 離散型データ事例 2.1 事例紹介：交通手段に関する調査 2.2 グラフィカル表現 2.3 確率的表現 2.4 パラメータの推定：条件付き確率表 2.5 DAG構造の学習：検定とスコア 2.6 離散型データでベイジアンネットワークを使ってみよう 2.7 ベイジアンネットワークの図示", " 2 離散型データ事例 2.1 事例紹介：交通手段に関する調査 項目名 略語 説明 年齢 Age(A) young:&lt;30, adult:30&lt;60, old:60&lt; 性別 Sex(S) male, female 教育水準 Education(E) high:高卒, uni:大卒 職業 Occupation(O) emp:従業員, self:自営業 居住地 Residence(R) small:小規模都市, big:大規模都市 交通手段 Travel(T) car, train, other 2.2 グラフィカル表現 6つの変数をノードとしてあらわす ノード間はアーク（矢印）で表現する 連続したアークの関係をパスと呼ぶ ベイジアンネットワークは循環のない、Directed acyclic graphs, DAGである必要がある library(bnlearn) #アーク無しのDAGを作成 dag &lt;- empty.graph(nodes=c(&quot;A&quot;,&quot;S&quot;,&quot;E&quot;,&quot;O&quot;,&quot;R&quot;,&quot;T&quot;)) dag ## ## Random/Generated Bayesian network ## ## model: ## [A][S][E][O][R][T] ## nodes: 6 ## arcs: 0 ## undirected arcs: 0 ## directed arcs: 0 ## average markov blanket size: 0.00 ## average neighbourhood size: 0.00 ## average branching factor: 0.00 ## ## generation algorithm: Empty #各アークを設定する dag &lt;- set.arc(dag, from=&quot;A&quot;, to=&quot;E&quot;) dag &lt;- set.arc(dag, from=&quot;S&quot;, to=&quot;E&quot;) dag &lt;- set.arc(dag, from=&quot;E&quot;, to=&quot;O&quot;) dag &lt;- set.arc(dag, from=&quot;E&quot;, to=&quot;R&quot;) dag &lt;- set.arc(dag, from=&quot;O&quot;, to=&quot;T&quot;) dag &lt;- set.arc(dag, from=&quot;R&quot;, to=&quot;T&quot;) dag ## ## Random/Generated Bayesian network ## ## model: ## [A][S][E|A:S][O|E][R|E][T|O:R] ## nodes: 6 ## arcs: 6 ## undirected arcs: 0 ## directed arcs: 6 ## average markov blanket size: 2.67 ## average neighbourhood size: 2.00 ## average branching factor: 1.00 ## ## generation algorithm: Empty graphviz.plot(dag) ノードやアークを確認することができる nodes(dag) ## [1] &quot;A&quot; &quot;S&quot; &quot;E&quot; &quot;O&quot; &quot;R&quot; &quot;T&quot; arcs(dag) ## from to ## [1,] &quot;A&quot; &quot;E&quot; ## [2,] &quot;S&quot; &quot;E&quot; ## [3,] &quot;E&quot; &quot;O&quot; ## [4,] &quot;E&quot; &quot;R&quot; ## [5,] &quot;O&quot; &quot;T&quot; ## [6,] &quot;R&quot; &quot;T&quot; まとめてアークを追加することも可能 dag2 &lt;- empty.graph(nodes = c(&quot;A&quot;,&quot;S&quot;,&quot;E&quot;,&quot;O&quot;,&quot;R&quot;,&quot;T&quot;)) arc.set &lt;- matrix(c(&quot;A&quot;,&quot;E&quot;, &quot;S&quot;,&quot;E&quot;, &quot;E&quot;,&quot;O&quot;, &quot;E&quot;,&quot;R&quot;, &quot;O&quot;,&quot;T&quot;, &quot;R&quot;,&quot;T&quot;), byrow =TRUE, ncol = 2, dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) arcs(dag2) &lt;- arc.set dag2 ## ## Random/Generated Bayesian network ## ## model: ## [A][S][E|A:S][O|E][R|E][T|O:R] ## nodes: 6 ## arcs: 6 ## undirected arcs: 0 ## directed arcs: 6 ## average markov blanket size: 2.67 ## average neighbourhood size: 2.00 ## average branching factor: 1.00 ## ## generation algorithm: Empty #同一か確認 all.equal(dag,dag2) ## [1] TRUE #循環している場合エラーがでる #set.arc(dag, from = &quot;T&quot;, to = &quot;E&quot;) 2.3 確率的表現 DAGに関する情報を使うことで、大域的分布（全変数の同時確率分布:多項分布）をより小さな局所的分布に分解可能 アークは直接的な依存関係を示すので、逆にアークがない変数間は条件付き独立となる \\[ Pr(A,S,E,O,R,T)=Pr(A)Pr(S)Pr(E|A,S)Pr(O|E)Pr(R|E)Pr(T|O,R) \\] 上記のように分解がうまくいくときはDAGが循環していないときである BNを作成するにあたり変数に同時確率分布を導入する必要あり すべて離散型データなのでRにおいて水準(level)という非連続状態のデータセットを定義する必要あり A.lv &lt;-c(&quot;young&quot;,&quot;adult&quot;,&quot;old&quot;) S.lv &lt;-c(&quot;M&quot;,&quot;F&quot;) E.lv &lt;-c(&quot;high&quot;,&quot;uni&quot;) O.lv &lt;-c(&quot;emp&quot;,&quot;self&quot;) R.lv &lt;-c(&quot;small&quot;,&quot;big&quot;) T.lv &lt;-c(&quot;car&quot;,&quot;train&quot;,&quot;other&quot;) A.prob &lt;-array(c(0.30,0.50,0.20), dim=3, dimnames = list(A = A.lv)) A.prob ## A ## young adult old ## 0.3 0.5 0.2 S.prob &lt;-array(c(0.60,0.40), dim=2, dimnames = list(S = S.lv)) S.prob ## S ## M F ## 0.6 0.4 O.prob &lt;-matrix(c(0.96,0.04,0.92,0.08), ncol = 2, dimnames = list(O=O.lv, E=E.lv)) O.prob ## E ## O high uni ## emp 0.96 0.92 ## self 0.04 0.08 R.prob &lt;-matrix(c(0.25,0.75,0.20,0.80), ncol = 2, dimnames = list(R=R.lv, E=E.lv)) R.prob ## E ## R high uni ## small 0.25 0.2 ## big 0.75 0.8 E.prob &lt;-array(c(0.75,0.25,0.72,0.28,0.88,0.12,0.64,0.36,0.70,0.30,0.90,0.10), dim = c(2,3,2), dimnames = list(E=E.lv, A=A.lv, S=S.lv)) E.prob ## , , S = M ## ## A ## E young adult old ## high 0.75 0.72 0.88 ## uni 0.25 0.28 0.12 ## ## , , S = F ## ## A ## E young adult old ## high 0.64 0.7 0.9 ## uni 0.36 0.3 0.1 T.prob &lt;-array(c(0.48,0.42,0.10,0.56,0.36,0.08,0.58,0.24,0.18,0.70,0.21,0.09), dim = c(3,2,2), dimnames = list(T=T.lv, O=O.lv, R=R.lv)) T.prob ## , , R = small ## ## O ## T emp self ## car 0.48 0.56 ## train 0.42 0.36 ## other 0.10 0.08 ## ## , , R = big ## ## O ## T emp self ## car 0.58 0.70 ## train 0.24 0.21 ## other 0.18 0.09 上記の条件付き確率表とDAGを組み合わせる必要あり #以下のように直接ネットワークを記述することも可能 dag3 &lt;- model2network(&quot;[A][S][E|A:S][O|E][R|E][T|O:R]&quot;) #cptは条件付き確率表を意味し、custom.fitでDAGと確率表を組み合わせられる cpt &lt;- list(A = A.prob, S = S.prob, E = E.prob, O = O.prob, R = R.prob, T= T.prob) bn &lt;- custom.fit(dag, cpt) #パラメータ数確認 nparams(bn) ## [1] 21 #arc確認 arcs(bn) ## from to ## [1,] &quot;A&quot; &quot;E&quot; ## [2,] &quot;S&quot; &quot;E&quot; ## [3,] &quot;E&quot; &quot;O&quot; ## [4,] &quot;E&quot; &quot;R&quot; ## [5,] &quot;O&quot; &quot;T&quot; ## [6,] &quot;R&quot; &quot;T&quot; #条件付き確率表を示せる bn$R ## ## Parameters of node R (multinomial distribution) ## ## Conditional probability table: ## ## E ## R high uni ## small 0.25 0.20 ## big 0.75 0.80 #条件付き確率表部分のみを出せる coef(bn$R) ## E ## R high uni ## small 0.25 0.20 ## big 0.75 0.80 #全体を示す bn ## ## Bayesian network parameters ## ## Parameters of node A (multinomial distribution) ## ## Conditional probability table: ## A ## young adult old ## 0.3 0.5 0.2 ## ## Parameters of node S (multinomial distribution) ## ## Conditional probability table: ## S ## M F ## 0.6 0.4 ## ## Parameters of node E (multinomial distribution) ## ## Conditional probability table: ## ## , , S = M ## ## A ## E young adult old ## high 0.75 0.72 0.88 ## uni 0.25 0.28 0.12 ## ## , , S = F ## ## A ## E young adult old ## high 0.64 0.70 0.90 ## uni 0.36 0.30 0.10 ## ## ## Parameters of node O (multinomial distribution) ## ## Conditional probability table: ## ## E ## O high uni ## emp 0.96 0.92 ## self 0.04 0.08 ## ## Parameters of node R (multinomial distribution) ## ## Conditional probability table: ## ## E ## R high uni ## small 0.25 0.20 ## big 0.75 0.80 ## ## Parameters of node T (multinomial distribution) ## ## Conditional probability table: ## ## , , R = small ## ## O ## T emp self ## car 0.48 0.56 ## train 0.42 0.36 ## other 0.10 0.08 ## ## , , R = big ## ## O ## T emp self ## car 0.58 0.70 ## train 0.24 0.21 ## other 0.18 0.09 2.4 パラメータの推定：条件付き確率表 survey &lt;- read.table(&quot;data/survey.txt&quot;, header = TRUE, colClasses = &quot;factor&quot;) head(survey) ## A R E O S T ## 1 adult big high emp F car ## 2 adult small uni emp M car ## 3 adult big uni emp F train ## 4 adult big high emp M car ## 5 adult big high emp M car ## 6 adult small high emp F train 離散型データの場合は、パラメータ=局所的分布における条件付き確率そのものとなる パラメータはデータセットにおける経験的頻度（実際の割合）から算出可能 \\[ \\widehat Pr(O=emp|E=high)=\\frac{\\widehat Pr(O=emp,E=high)}{\\widehat Pr(E=high)}=\\frac{O=empかつE=highの数}{E=highの数} \\] 古典的な頻度主義や最尤推定に関連する式ともいえる #bn.fit関数を用いることでデータからパラメータ推定可能 #mleは最尤推定法を用いている bn.mle &lt;- bn.fit(dag, data = survey, method = &quot;mle&quot;) bn.mle$O ## ## Parameters of node O (multinomial distribution) ## ## Conditional probability table: ## ## E ## O high uni ## emp 0.98082192 0.92592593 ## self 0.01917808 0.07407407 #パッケージを使わずに頻度を計算してみる head(survey[, c(&quot;O&quot;, &quot;E&quot;)]) ## O E ## 1 emp high ## 2 emp uni ## 3 emp uni ## 4 emp high ## 5 emp high ## 6 emp high #tableでカウントして集計してくれる table(survey[, c(&quot;O&quot;, &quot;E&quot;)]) ## E ## O high uni ## emp 358 125 ## self 7 10 #prop.tableで頻度を計算できる、marginはたぶん桁数関連 prop.table(table(survey[, c(&quot;O&quot;, &quot;E&quot;)]), margin = 2) ## E ## O high uni ## emp 0.98082192 0.92592593 ## self 0.01917808 0.07407407 上記結果がbn.fitで算出したものと同一であることがわかる #bayesにすると事後分布を用いたベイズ的方法になる bn.bayes &lt;- bn.fit(dag, data = survey, method = &quot;bayes&quot;, iss = 10) bn.bayes$O ## ## Parameters of node O (multinomial distribution) ## ## Conditional probability table: ## ## E ## O high uni ## emp 0.97432432 0.91071429 ## self 0.02567568 0.08928571 iss (imaginary sample size)はオプション：事前分布にどの程度重み付けするか 小さい値（1-15）にするのが一般的、値が大きいと事後分布が一様になり事前分布として用いられた一様分布へと近似していく ベイズのほうがより1から遠い値となる→0を含むセルが減る 最尤推定法よりもロバストで予測力の高いベイジアンネットワークを構築可能 2.5 DAG構造の学習：検定とスコア DAGの構造を探索していくこと自体が調査の目的の場合もある どのノードが分析対象のノードと直接関連があるか特定可能 2.5.1 条件付き独立性検定 個々のアークの有無に焦点を当てたもの 条件付き独立の帰無仮説（確率的に独立である）が棄却されるならそのアークをDAGの中に加えることができる #ci.test関数で対数尤度比検定、Χ2検定が可能 ci.test(&quot;T&quot;,&quot;E&quot;,c(&quot;O&quot;,&quot;R&quot;),test = &quot;mi&quot;, data = survey) ## ## Mutual Information (disc.) ## ## data: T ~ E | O + R ## mi = 9.8836, df = 8, p-value = 0.2733 ## alternative hypothesis: true value is greater than 0 ci.test(&quot;T&quot;,&quot;E&quot;,c(&quot;O&quot;,&quot;R&quot;),test = &quot;x2&quot;, data = survey) ## ## Pearson&#39;s X^2 ## ## data: T ~ E | O + R ## x2 = 8.2375, df = 8, p-value = 0.4106 ## alternative hypothesis: true value is greater than 0 いずれもp値が大きいため、E→Tの関連性で有意差なし→現在のDAG構造に加えるような関連性なし #まとめて検定を実施可能 arc.strength(dag, data = survey, criterion = &quot;x2&quot;) ## from to strength ## 1 A E 0.0009777168 ## 2 S E 0.0012537013 ## 3 E O 0.0026379469 ## 4 E R 0.0005599201 ## 5 O T 0.4339127237 ## 6 R T 0.0013584250 O→T以外のすべてのアークは支持されたものと判断可能 2.5.2 ネットワークスコア ネットワーク全体としてのDAGに焦点を当てている。 DAGがデータの依存構造をどの程度よく反映しているかの適合度指標 \\[ BIC = \\log\\widehat{Pr}(A,S,E,O,R,T) - \\frac{d}{2}\\log n \\] n：サンプルサイズ、d：ネットワーク全体のパラメータ数 DAGがデータにフィットしているほど高い値を示す #BIC score(dag, data = survey, type = &quot;bic&quot;) ## [1] -2012.687 #対数BDe score(dag, data = survey, type = &quot;bde&quot;, iss = 10) ## [1] -1998.284 #例としてランダムグラフを作ってみるとさすがにスコアが悪い rnd &lt;- random.graph(nodes = c(&quot;A&quot;,&quot;S&quot;,&quot;E&quot;,&quot;O&quot;,&quot;R&quot;,&quot;T&quot;)) modelstring(rnd) ## [1] &quot;[A][S][E|S][O|A][R|S:E:O][T|S:O]&quot; score(rnd, data = survey, type = &quot;bic&quot;) ## [1] -2033.747 ネットワークのスコアが最大となるDAGを探索するためのアルゴリズム 山登り法 アークなしのDAGからスタートして1つひとつのアークを順次追加、除去、反転させることで最もネットワークスコアが増加する状況を探索する方法 hcを使ったらデフォルトはbicで計算される learned &lt;- hc(survey) modelstring(learned) ## [1] &quot;[R][E|R][T|R][A|E][O|E][S|E]&quot; score(learned, data = survey, type = &quot;bic&quot;) ## [1] -1998.432 #各アークを取り除いたときどれくらいスコアに影響があるかを計算できる #R→Eが重要であることが分かる arc.strength(learned, data = survey, criterion = &quot;bic&quot;) ## from to strength ## 1 R E -3.3896261 ## 2 E S -2.7260640 ## 3 R T -1.8484171 ## 4 E A -1.7195441 ## 5 E O -0.8266937 #ただし規定していたdagを用いたときと異なっており、surveyデータのみではすべての依存関係が正確に学習できていないことを示している arc.strength(dag, data = survey, criterion = &quot;bic&quot;) ## from to strength ## 1 A E 2.4889383 ## 2 S E 1.4824183 ## 3 E O -0.8266937 ## 4 E R -3.3896261 ## 5 O T 10.0457874 ## 6 R T 2.9734338 2.6 離散型データでベイジアンネットワークを使ってみよう 2.6.1 DAG構造を使って dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;) ## [1] FALSE 教育水準(E)は性別(S)から影響を受けており、居住地(R)は教育水準(E)から影響を受けている（S→E、E→R）ので 性別(S)と居住地(R)が関連することは明らか 教育水準(E)を条件付けると性別(S)と居住地(R)の間のパスをブロックすることになるので独立になる dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;, z = &quot;E&quot;) ## [1] TRUE \\[ Pr(O,R|E) = Pr(O|E)Pr(R|E) \\] 2.6.2 条件付き確率表を使って 2.6.2.1 厳密推論 library(gRain) ベイジアンネットワークを特別丹念に構築されたツリー構造に変換する方法に依拠している ツリー構造に対して、compile関数で確率表を計算できる→setEvicence関数を用いてエビデンスをjunctionオブジェクトに入力する 例）「女性が自動車や電車を利用することに対する態度」を調査したい。男女のサンプルvs女性だけのサンプルで比較したい #全体サンプル junction &lt;- compile(as.grain(bn)) querygrain(junction, nodes = &quot;T&quot;)$T ## T ## car train other ## 0.5618340 0.2808573 0.1573088 #女性だけのサンプル jsex &lt;- setEvidence(junction, nodes = &quot;S&quot;, states = &quot;F&quot;) querygrain(jsex, nodes = &quot;T&quot;)$T ## T ## car train other ## 0.5620577 0.2806144 0.1573280 →好みは同程度であった 例2）小規模の都市に居住することで利用する交通手段がどうかわるか jres &lt;- setEvidence(junction, nodes = &quot;R&quot;, states = &quot;small&quot;) querygrain(jres, nodes = &quot;T&quot;)$T ## T ## car train other ## 0.48388675 0.41708494 0.09902831 条件付き独立性を評価したい 例3）教育水準が与えられた場合の性別と交通手段の同時確率分布 jedu &lt;- setEvidence(junction, nodes = &quot;E&quot;, states = &quot;high&quot;) SxT.cpt &lt;- querygrain(jedu, nodes = c(&quot;S&quot;,&quot;T&quot;), type = &quot;conditional&quot;) SxT.cpt ## T ## S car train other ## M 0.612557 0.612557 0.612557 ## F 0.387443 0.387443 0.387443 2番目のノードで条件付けられた場合の1番目のノードの分布を算出できた 交通手段(T)がどのような状態でも男性にかかる条件付き確率は同じ→教育水準Eのもとで性別Sと交通手段Tは独立 2.6.2.2 近似推論 ベイジアンネットワークを利用することで観測値をランダムに生成する方法（モンテカルロシミュレーション） 計算コストは高いが、多くのノードを含む大規模なBNを扱うことが可能 #あるエビデンスを与えた場合の特定のイベントに関する確率を算出する cpquery(bn, event = (S == &quot;M&quot;) &amp; (T == &quot;car&quot;), evidence = (E == &quot;high&quot;)) ## [1] 0.3397436 ただしquerygrain関数で出される正確な値とは多少異なる #nを増やせば多少改善はするが時間がかかる cpquery(bn, event = (S == &quot;M&quot;) &amp; (T == &quot;car&quot;), evidence = (E == &quot;high&quot;), n = 10^6) ## [1] 0.3423342 #尤度重み付き方法を使えば真値に非常に近い確率で算出可能 cpquery(bn, event = (S == &quot;M&quot;) &amp; (T == &quot;car&quot;), evidence = list(E = &quot;high&quot;), method = &quot;lw&quot;) ## [1] 0.3527989 #cpdist関数はエビデンスに適合するような変数のランダムな観測値を算出し、それを含んだdfを返す SxT &lt;- cpdist(bn, nodes = c(&quot;S&quot;, &quot;T&quot;), evidence = (E == &quot;high&quot;)) head(SxT) ## S T ## 1 M train ## 2 F car ## 3 F car ## 4 F car ## 5 M car ## 6 F train 2.7 ベイジアンネットワークの図示 詳細は以下に記載あり https://www.bnlearn.com/examples/graphviz-plot/ graphviz.plot(dag) hlight &lt;- list(nodes = nodes(dag), arcs = arcs(dag), col = &quot;grey&quot;, textCol = &quot;grey&quot;) pp &lt;- graphviz.plot(dag, highlight = hlight, render = FALSE) library(Rgraphviz) edgeRenderInfo(pp) &lt;- list(col = c(&quot;S~E&quot; =&quot;black&quot;, &quot;E~R&quot; = &quot;black&quot;), lwd = c(&quot;S~E&quot; = 3, &quot;E~R&quot; = 3)) nodeRenderInfo(pp) &lt;- list(col = c(&quot;S&quot; = &quot;black&quot;, &quot;E&quot; = &quot;black&quot;, &quot;R&quot; = &quot;black&quot;), textCol = c(&quot;S&quot; = &quot;black&quot;, &quot;E&quot; = &quot;black&quot;, &quot;R&quot; = &quot;black&quot;), fill = c(&quot;E&quot; = &quot;grey&quot;)) renderGraph(pp) 2.7.1 条件付き確率分布の図示 bn.fit.barchart(bn.mle$T, main = &quot;Travel&quot;, xlab = &quot;Pr(T|R,O)&quot;, ylab =&quot;&quot;) 交通手段の周辺確率と2つの条件付き確率クエリの結果を比較する Evidence &lt;- factor(c(rep(&quot;Unconditional&quot;,3), rep(&quot;Female&quot;, 3), rep(&quot;Small City&quot;,3)), levels = c(&quot;Unconditional&quot;, &quot;Female&quot;, &quot;Small City&quot;)) Travel &lt;- factor(rep(c(&quot;car&quot;, &quot;train&quot;, &quot;other&quot;), 3), levels = c(&quot;other&quot;, &quot;train&quot;, &quot;car&quot;)) distr &lt;- data.frame(Evidence = Evidence, Travel = Travel, Prob = c(0.5618, 0.2808, 0.15730, 0.5620, 0.2806, 0.1573, 0.4838, 0.4170, 0.0990)) distr ## Evidence Travel Prob ## 1 Unconditional car 0.5618 ## 2 Unconditional train 0.2808 ## 3 Unconditional other 0.1573 ## 4 Female car 0.5620 ## 5 Female train 0.2806 ## 6 Female other 0.1573 ## 7 Small City car 0.4838 ## 8 Small City train 0.4170 ## 9 Small City other 0.0990 library(lattice) barchart(Travel ~ Prob | Evidence, data = distr, layout = c(3, 1), xlab = &quot;probability&quot;, scales = list(alternating = 1, tck = c(1, 0)), strip = strip.custom(factor.levels = c(expression(Pr(T)), expression(Pr({T} * &quot; | &quot; * {S == F})), expression(Pr({T} * &quot; | &quot; * {R == small})))), panel = function(...) { panel.barchart(...) panel.grid(h = 0, v = -1) }) オリジナルのベイジアンネットワークとエビデンスありのベイジアンネットワークを比較 仕様書 https://www.bnlearn.com/documentation/man/graphviz.chart.html library(&quot;Rgraphviz&quot;) graphviz.chart(bn) graphviz.chart(as.bn.fit(jedu, including.evidence = TRUE), grid = TRUE, bar.col = c(A = &quot;black&quot;, S = &quot;black&quot;, E = &quot;grey&quot;, O = &quot;black&quot;, R = &quot;black&quot;, T = &quot;black&quot;), strip.bg = c(A = &quot;transparent&quot;, S = &quot;transparent&quot;, E = &quot;grey&quot;, O = &quot;transparent&quot;, R = &quot;transparent&quot;, T = &quot;transparent&quot;), main = &quot;BN with Evidence&quot;) "],["連続型データ事例ガウシアンベイジアンネットワーク.html", "3 連続型データ事例：ガウシアン・ベイジアンネットワーク 3.1 事例紹介 3.2 グラフィカル表現 3.3 確率的表現 3.4 パラメータの推定：相関係数 3.5 DAG構造の学習：検定とスコア 3.6 ガウシアン・ベイジアンネットワークを使ってみよう 3.7 ガウシアン・ベイジアンネットワークの図示", " 3 連続型データ事例：ガウシアン・ベイジアンネットワーク 3.1 事例紹介 いずれも連続型変数である 項目名 略語 説明 遺伝的ポテンシャル G 環境的ポテンシャル E 栄養器官 V ある単一の植物に蓄えられる栄養に関する情報すべてを統合した変数 種子の数 N 開花時期には判明する 種子の平均重量 W 植物の寿命の後期にならないとわからない 作物の収穫量 C 3.2 グラフィカル表現 library(bnlearn) dag.bnlearn &lt;- model2network(&quot;[G][E][V|G:E][N|V][W|V][C|N:W]&quot;) dag.bnlearn ## ## Random/Generated Bayesian network ## ## model: ## [E][G][V|E:G][N|V][W|V][C|N:W] ## nodes: 6 ## arcs: 6 ## undirected arcs: 0 ## directed arcs: 6 ## average markov blanket size: 2.67 ## average neighbourhood size: 2.00 ## average branching factor: 1.00 ## ## generation algorithm: Empty graphviz.plot(dag.bnlearn) #どの変数の組み合わせが周辺的に独立しているか確認する crop.nodes &lt;- nodes(dag.bnlearn) for(n1 in crop.nodes){ for(n2 in crop.nodes){ if(dsep(dag.bnlearn, n1, n2)) cat(n1, &quot;and&quot;, n2, &quot;are independent.\\n&quot;) } } ## E and G are independent. ## G and E are independent. EとGのみが独立している、独立性は対称的だった #変数をすべての組あわせで対にして、栄養器官(V)で条件づけられた場合にどのペアが条件付き独立となるか確認できる for(n1 in crop.nodes[crop.nodes != &quot;V&quot;]){ for(n2 in crop.nodes[crop.nodes != &quot;V&quot;]){ if(n1&lt;n2){ if(dsep(dag.bnlearn, n1, n2, &quot;V&quot;)) cat(n1, &quot;and&quot;, n2, &quot;are independent given V.\\n&quot;) } } } ## C and E are independent given V. ## C and G are independent given V. ## E and N are independent given V. ## E and W are independent given V. ## G and N are independent given V. ## G and W are independent given V. ## N and W are independent given V. Vで条件づけられるとEとGは独立ではない 3.3 確率的表現 #各パラメータの確率分布を定義する E.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = 50), sd = 10) G.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = 50), sd = 10) V.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = -10.35534, E = 0.70711, G = 0.5), sd = 5) N.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = 45, V = 0.1), sd = 9.949874) W.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = 15, V = 0.7), sd = 7.141428) C.dist &lt;- list(coef = c(&quot;(Intercept)&quot; = 0, N = 0.3, W = 0.7), sd = 6.25) dist.list &lt;- list(E = E.dist, G = G.dist, V = V.dist, N = N.dist, W = W.dist, C = C.dist) gbn.bnlearn &lt;- custom.fit(dag.bnlearn, dist = dist.list) #局所分布のパラメータ gbn.bnlearn$C ## ## Parameters of node C (Gaussian distribution) ## ## Conditional density: C | N + W ## Coefficients: ## (Intercept) N W ## 0.0 0.3 0.7 ## Standard deviation of the residuals: 6.25 上記は線形ガウシアン・ベイジアンネットワークである すべてのノードは正規分布に基づく 親のノードをもたないノードは周辺分布により記述 各ノードの分散はそのノードに特有で親ノードに依存しない 各ノードの局所的分布は切片と親ノードを含んだ線形ガウシアンモデルとして表現 #ガウシアン・ベイジアンネットワークに特化したRパッケージ library(rbmn) #bn.fitオブジェクトを変換する gbn.rbmn &lt;- bnfit2nbn(gbn.bnlearn) #多変量正規分布のパラメータを得る gema.rbmn &lt;- nbn2gema(gbn.rbmn) mn.rbmn &lt;- gema2mn(gema.rbmn) print8mn(mn.rbmn) ## mu s.d. C.E C.G C.V C.W C.N C.C ## E 50 10 1.000 0.00 0.707 0.495 0.071 0.368 ## G 50 10 0.000 1.00 0.500 0.350 0.050 0.260 ## V 50 10 0.707 0.50 1.000 0.700 0.100 0.520 ## W 50 10 0.495 0.35 0.700 1.000 0.070 0.721 ## N 50 10 0.071 0.05 0.100 0.070 1.000 0.349 ## C 50 10 0.368 0.26 0.520 0.721 0.349 1.000 1列目：周辺分布の期待値 2列目：周辺分布の標準偏差 3列目以降：相関行列 3.4 パラメータの推定：相関係数 DAG構造は既知とする #200の観測サンプルを生成して、cropdata200に格納する set.seed(4567) cropdata200 &lt;- rbn(gbn.bnlearn, n = 200) set.seed(1234) cropdata20k &lt;- rbn(gbn.bnlearn, n = 20000) dim(cropdata200) ## [1] 200 6 round(head(cropdata200), 2) ## C E G N V W ## 1 48.83 51.48 42.64 54.10 42.96 41.96 ## 2 48.85 73.43 40.97 60.07 65.29 48.96 ## 3 67.01 71.10 52.52 51.64 63.22 62.03 ## 4 37.83 49.33 56.15 49.01 47.75 38.77 ## 5 55.30 49.27 63.55 54.62 60.57 56.66 ## 6 56.12 48.72 66.02 43.95 55.54 52.39 #パラメータを推定する #変数が因子型でなければ自動的にガウシアン・ベイジアンネットワークと判断される crop.fitted &lt;- bn.fit(dag.bnlearn, data = cropdata200) crop.fitted ## ## Bayesian network parameters ## ## Parameters of node C (Gaussian distribution) ## ## Conditional density: C | N + W ## Coefficients: ## (Intercept) N W ## 2.4026177 0.2734476 0.6858202 ## Standard deviation of the residuals: 6.31327 ## ## Parameters of node E (Gaussian distribution) ## ## Conditional density: E ## Coefficients: ## (Intercept) ## 50.80161 ## Standard deviation of the residuals: 10.7432 ## ## Parameters of node G (Gaussian distribution) ## ## Conditional density: G ## Coefficients: ## (Intercept) ## 50.14396 ## Standard deviation of the residuals: 9.798923 ## ## Parameters of node N (Gaussian distribution) ## ## Conditional density: N | V ## Coefficients: ## (Intercept) V ## 51.81745022 -0.04461524 ## Standard deviation of the residuals: 9.531253 ## ## Parameters of node V (Gaussian distribution) ## ## Conditional density: V | E + G ## Coefficients: ## (Intercept) E G ## -10.4547647 0.7426946 0.4552872 ## Standard deviation of the residuals: 5.034309 ## ## Parameters of node W (Gaussian distribution) ## ## Conditional density: W | V ## Coefficients: ## (Intercept) V ## 20.1555287 0.5899971 ## Standard deviation of the residuals: 6.792171 #特定のノードについて異なるパラメータ推定法を用いることもできる crop.fitted$C &lt;- lm(C ~ N + W, data = cropdata200) crop.fitted$C ## ## Parameters of node C (Gaussian distribution) ## ## Conditional density: C | N + W ## Coefficients: ## (Intercept) N W ## 2.4026177 0.2734476 0.6858202 ## Standard deviation of the residuals: 6.31327 一般的な回帰モデルをもちいてパラメータ推定を行う リッジ、ラッソ回帰などについては以下を参照 https://aizine.ai/ridge-lasso-elasticnet/ #収穫量Cについてリッジ回帰を当てはめてみる library(penalized) crop.fitted$C &lt;- penalized(C ~ N + W, lambda1 = 0, lambda2 = 1.5, data = cropdata200) ## 1\b2\b 推定した値を真値（avg:50, sd:10)と比較する crop.fitted$E ## ## Parameters of node E (Gaussian distribution) ## ## Conditional density: E ## Coefficients: ## (Intercept) ## 50.80161 ## Standard deviation of the residuals: 10.7432 crop.fitted$C ## ## Parameters of node C (Gaussian distribution) ## ## Conditional density: C | N + W ## Coefficients: ## (Intercept) N W ## 2.4069159 0.2734233 0.6857580 ## Standard deviation of the residuals: 6.31327 NやW、SDについては真値に近似しているが、切片が0とは遠いので以下で直接修正する crop.fitted$C &lt;- lm(C ~ N + W - 1, data = cropdata200) crop.fitted$C ## ## Parameters of node C (Gaussian distribution) ## ## Conditional density: C | N + W ## Coefficients: ## (Intercept) N W ## 0.0000000 0.2963220 0.7105197 ## Standard deviation of the residuals: 6.304969 3.5 DAG構造の学習：検定とスコア 3.5.1 条件付き独立性検定 cormat &lt;- cor(cropdata200[, c(&quot;C&quot;, &quot;W&quot;, &quot;N&quot;)]) cormat ## C W N ## C 1.0000000 0.67479818 0.26480613 ## W 0.6747982 1.00000000 -0.02597999 ## N 0.2648061 -0.02597999 1.00000000 #cormat（相関行列）からinvcor(偏相関行列)を計算する #ほかのすべての変数で条件づけられた場合のXとYにおける偏相関という意味 library(corpcor) invcor &lt;- cor2pcor(cormat) dimnames(invcor) &lt;- dimnames(cormat) invcor ## C W N ## C 1.0000000 0.7071522 0.3826989 ## W 0.7071522 1.0000000 -0.2875974 ## N 0.3826989 -0.2875974 1.0000000 #種子の数Nで条件づけられた場合に作物の収穫量Cと種子の平均重量Wは独立か、という仮説を検証できる ci.test(&quot;C&quot;, &quot;W&quot;, &quot;N&quot;, test = &quot;cor&quot;, data = cropdata200) ## ## Pearson&#39;s Correlation ## ## data: C ~ W | N ## cor = 0.70715, df = 197, p-value &lt; 2.2e-16 ## alternative hypothesis: true value is not equal to 0 有意な偏相関があったことから帰無仮説を棄却し、独立でないと言える #200個のデータセットは大規模ではないので構造自体を学習させると以下のようになる pdag1 &lt;- iamb(cropdata200, test = &quot;cor&quot;) graphviz.plot(pdag1) #20kのデータセットを用いると正しく予測できる pdag2 &lt;- iamb(cropdata20k, test = &quot;cor&quot;) graphviz.plot(pdag2) #一部アークの追加、削除ができる wl &lt;- matrix(c(&quot;V&quot;, &quot;N&quot;), ncol = 2) pdag3 &lt;- iamb(cropdata200, test = &quot;cor&quot;, whitelist = wl) graphviz.plot(pdag3) 3.5.2 ネットワークスコア \\[ BIC = \\log\\widehat{f}(E,G,V,N,W,C) - \\frac{d}{2}\\log n \\] また事後確率のスコアとしてBGe(Bayesian Gaussian equivalent score) score(dag.bnlearn, data = cropdata20k, type = &quot;bic-g&quot;) ## [1] -416421.2 score(dag.bnlearn, data = cropdata20k, type = &quot;bge&quot;) ## [1] -416494.5 3.6 ガウシアン・ベイジアンネットワークを使ってみよう 3.6.1 厳密推論 #まずどのようにBNが定義されているかを確認する print8nbn(gbn.rbmn) ## =====Nodes===[parents] = Exp. (sd.dev) ## ----------------------------------------------- ## ---------E---[-] = 50 (10) ## ---------G---[-] = 50 (10) ## ---------V---[E,G] = -10.355 + 0.707*E + 0.5*G (5) ## ---------W---[V] = 15 + 0.7*V (7.141) ## ---------N---[V] = 45 + 0.1*V (9.95) ## ---------C---[N,W] = 0.3*N + 0.7*W (6.25) print8gema(gema.rbmn) ## mu E1 E2 E3 E4 E5 E6 ## E 50 10.000 0.0 0.0 0.000 0.000 0.00 ## G 50 0.000 10.0 0.0 0.000 0.000 0.00 ## V 50 7.071 5.0 5.0 0.000 0.000 0.00 ## W 50 4.950 3.5 3.5 7.141 0.000 0.00 ## N 50 0.707 0.5 0.5 0.000 9.950 0.00 ## C 50 3.677 2.6 2.6 4.999 2.985 6.25 別のノードの値を固定した場合の1つor複数のノードの条件付同時分布を算出可能 #Vを80に固定した場合のCの分布 print8mn(condi4joint(mn.rbmn, par = &quot;C&quot;, pour = &quot;V&quot;, x2 = 80)) ## mu s.d. ## C 65.6 8.542 3.6.2 近似推論 シミュレーションを用いる 直接的：rnb関数 制約的：cpquery/cpdist関数 #(V,N)から観測値4つを生成する nobs &lt;- 4 sim &lt;- rbn(gbn.bnlearn, n = nobs) sim ## C E G N V W ## 1 65.41210 79.12004 67.14710 63.82861 79.28153 59.53187 ## 2 46.01587 35.57628 59.05068 51.74454 37.51262 40.01539 ## 3 63.36558 62.26097 48.24989 53.12280 57.59524 64.20510 ## 4 40.36104 35.87449 59.02320 37.77818 50.75239 47.35855 #条件を付けて生成する #非常に良質な作物を生産するための種子の数や平均重量はどのようなものか head(cpdist(gbn.bnlearn, nodes = c(&quot;C&quot;, &quot;N&quot;, &quot;W&quot;), evidence = (C &gt; 80))) ## C N W ## 1 82.73325 66.14010 73.51469 ## 2 81.32917 66.46854 80.87893 ## 3 83.71564 64.03459 75.02483 ## 4 81.56583 55.56495 63.87066 ## 5 81.67702 65.23859 65.27324 ## 6 80.12171 59.03308 72.62500 “=”を使って単一値を条件づける場合はこのアプローチは不可能となる 連続型分布において単一値はつねに確率がゼロであるから ただし以下のように尤度重み付け法を用いることで解決することができる head(cpdist(gbn.bnlearn, nodes = c(&quot;V&quot;,&quot;G&quot;,&quot;E&quot;), evidence = list(G = 10, E = 90), method = &quot;lw&quot;), n = 5) ## V G E ## 1 63.71504 10 90 ## 2 62.78473 10 90 ## 3 67.95987 10 90 ## 4 53.36817 10 90 ## 5 58.42986 10 90 #また特定のイベントに関する確率を算出できる cpquery(gbn.bnlearn, event = (V &gt; 70), evidence = list(G = 10, E = 90), method = &quot;lw&quot;) ## [1] 0.009666667 3.7 ガウシアン・ベイジアンネットワークの図示 3.7.1 DAGの図示 #別の方法を用いて作図してみる library(igraph) igraph.options(print.full = TRUE) dag0.igraph &lt;- graph.formula(G-+V, E-+V, V-+N, V-+W, N-+C, W-+C) dag0.igraph ## IGRAPH e8b842b DN-- 6 6 -- ## + attr: name (v/c) ## + edges from e8b842b (vertex names): ## [1] G-&gt;V V-&gt;N V-&gt;W E-&gt;V N-&gt;C W-&gt;C #bnから変換もできる dag.igraph &lt;- as.igraph(dag.bnlearn) #ノードを表す V(dag.igraph) ## + 6/6 vertices, named, from e8bdb08: ## [1] C E G N V W #エッジを表す E(dag.igraph) ## + 6/6 edges from e8bdb08 (vertex names): ## [1] G-&gt;V E-&gt;V V-&gt;N V-&gt;W N-&gt;C W-&gt;C par(mfrow = c(1, 3), mar = rep(3, 4), cex.main = 2) plot(dag.igraph, main = &quot;\\n1: defaults&quot;) ly &lt;- matrix(c(2, 3, 1, 1, 2, 3, 1, 4, 4, 2, 3, 2), 6) plot(dag.igraph, layout = ly, main = &quot;\\n2: positioning&quot;) vcol &lt;- c(&quot;black&quot;, &quot;darkgrey&quot;, &quot;darkgrey&quot;, rep(NA, 3)) lcol &lt;- c(rep(&quot;white&quot;, 3), rep(NA, 3)) par(mar = rep(0, 4), lwd = 1.5) plot(dag.igraph, layout = ly, frame = TRUE, main = &quot;\\n3: final&quot;, vertex.color = vcol, vertex.label.color = lcol, vertex.label.cex = 3, vertex.size = 50, edge.arrow.size = 0.8, edge.color = &quot;black&quot;) 3.7.2 条件付き確率分布の図示 ガウシアン・ベイジアンネットワークの局所的分布を図示するための関数はない 線形回帰モデルのための回帰診断図を利用することは可能 ただし、残差と当てはめ値がbn.fitオブジェクトに格納されている必要がある gbn.fit &lt;- bn.fit(dag.bnlearn, data = cropdata20k) #残差に関するquantile-quantileプロット bn.fit.qqplot(gbn.fit) #理論上の正規密度分布と重ねた残差のヒストグラム bn.fit.histogram(gbn.fit) #当てはめ値に対する残差プロット bn.fit.xyplot(gbn.fit) #一つのノードのみに着目した図もかける bn.fit.qqplot(gbn.fit$C) 環境的ポテンシャルEと栄養期間Vの組み合わせに対してどのように収穫量Cが変化するのかについて興味があるとする（C|E,V) C.EV &lt;- condi4joint(mn.rbmn, par = &quot;C&quot;, pour = c(&quot;E&quot;, &quot;V&quot;), x2 = NULL) C.EV$rho ## E V ## C 0 0.52 Vに関する情報が得られたときはEに関する回帰係数が0になる→Cを推定するに当たりEに関する情報は役に立たない #VがEとCと有効分離の関係にある dsep(gbn.bnlearn, &quot;E&quot;,&quot;C&quot;,&quot;V&quot;) ## [1] TRUE EとVの両方を変化させた場合のCに関する分布を算出したい 3次元プロットは難しくエラーが発生しやすいので、二次元で観測値のポイントの大きさを変えることで表現する set.seed(5678) cropdata3 &lt;- cpdist(gbn.bnlearn, nodes = c(&quot;E&quot;, &quot;V&quot;, &quot;C&quot;), evidence = TRUE, n = 1000) plot(cropdata3$V, cropdata3$C, type = &quot;n&quot;, main = &quot;C|V,E; E is the point size&quot;) cexlim &lt;- c(0.1, 2.4) cexE &lt;- cexlim[1] + diff(cexlim) / diff(range(cropdata3$E) * (cropdata3$E - min(cropdata3$E))) points(cropdata3$V, cropdata3$C, cex = cexE*10) cqa &lt;- quantile(cropdata3$C, seq(0, 1, 0.1)) abline(h = cqa, lty = 3) "],["混合事例条件付きガウシアンベイジアンネットワーク.html", "4 混合事例：条件付きガウシアン・ベイジアンネットワーク 4.1 事例紹介：健康管理にかかるコスト 4.2 パラメータの推定：混合回帰 4.3 DAG構造の学習：検定とスコア 4.4 条件付きガウシアンベイジアンネットワークを使ってみよう", " 4 混合事例：条件付きガウシアン・ベイジアンネットワーク 4.1 事例紹介：健康管理にかかるコスト 項目名 型 略語 説明 年齢 離散 A young, adult, old 既往歴 離散 C none, mild, severe 通院に係る支出 連続 O 専門相談員のところに通院する際に係る個人の支出 入院に係る支出 連続 I 入院全般にかかる個人の費用 入院の有無 離散 H any:数日過ごした, none:入院無し 入院日数 連続 D 医療費に係る税金 連続 T 医療費を支払う際にかかる税金 library(bnlearn) dag &lt;- model2network(&quot;[A][C|A][H|A][D|A:H][I|C:D][O|A][T|O:I]&quot;) graphviz.plot(dag) #Aの条件付き確率表作成 A.lv &lt;- c(&quot;young&quot;, &quot;adult&quot;, &quot;old&quot;) A.prob &lt;- array(c(0.35, 0.45, 0.20), dim = 3, dimnames = list(A = A.lv)) A.prob ## A ## young adult old ## 0.35 0.45 0.20 年齢に応じて既往症のレベルが異なるようにCを条件付ける C.lv &lt;- c(&quot;none&quot;, &quot;mild&quot;, &quot;severe&quot;) C.prob &lt;- array(c(0.88, 0.10, 0.02, 0.75, 0.20, 0.05, 0.42, 0.53, 0.05), dim = c(3, 3), dimnames = list(C = C.lv, A = A.lv)) C.prob ## A ## C young adult old ## none 0.88 0.75 0.42 ## mild 0.10 0.20 0.53 ## severe 0.02 0.05 0.05 H.lv &lt;- c(&quot;none&quot;, &quot;any&quot;) H.prob &lt;- array(c(0.90, 0.10, 0.75, 0.25, 0.60, 0.40), dim = c(2, 3), dimnames = list(H = H.lv, A = A.lv)) H.prob ## A ## H young adult old ## none 0.9 0.75 0.6 ## any 0.1 0.25 0.4 D.coef &lt;- list(coef = array(c(0, 0, 0, 1, 4, 7), dim = c(1, 6), dimnames = list(&quot;(Intercept)&quot;, NULL)), sd = c(0, 0, 0, 0.5, 1, 1.5)) D.coef ## $coef ## [,1] [,2] [,3] [,4] [,5] [,6] ## (Intercept) 0 0 0 1 4 7 ## ## $sd ## [1] 0.0 0.0 0.0 0.5 1.0 1.5 DはH(入院の有無)×A(young mild old)に対応している Hがnoneなら0 離散型変数→連続型変数の場合 親ノードの離散型変数の値に応じて複数の線形回帰モデルをセットとして表現する 連続型変数が複数の離散型変数を親ノードとして持つ場合 親ノードの値に対応した回帰モデルを複数含む形で構成される 連続型変数の親ノードが連続型変数である場合 親ノードの連続型変数が子ノードの下位モデルに混在する形 I.coef &lt;- list(coef = array(c(1, 3, 1, 5.5, 1, 8) * 100 , dim = c(2, 3), dimnames = list(c(&quot;(Intercept)&quot;, &quot;D&quot;), NULL)) , sd = c(30, 50, 100)) I.coef ## $coef ## [,1] [,2] [,3] ## (Intercept) 100 100 100 ## D 300 550 800 ## ## $sd ## [1] 30 50 100 O.coef &lt;- list(coef = array(c(60, 180, 360), dim = c(1, 3), dimnames = list(&quot;(Intercept)&quot;, NULL)), sd = c(10, 20, 40)) T.coef &lt;- list(coef = c(&quot;(Intercept)&quot; = 120, I = 1.02, O = 1.05), sd = 10) \\[ T= 120+1.02I+1.05O+\\varepsilon_T\\sim N(0,10^2) \\] 上記内容でBNを定義したのでbn.fitオブジェクトを作成する dists &lt;- list(A = A.prob, C= C.prob, H = H.prob, D= D.coef, I = I.coef, O = O.coef, &quot;T&quot; = T.coef) healthcare &lt;- custom.fit(dag, dists) healthcare$I ## ## Parameters of node I (conditional Gaussian distribution) ## ## Conditional density: I | C + D ## Coefficients: ## 0 1 2 ## (Intercept) 100 100 100 ## D 300 550 800 ## Standard deviation of the residuals: ## 0 1 2 ## 30 50 100 ## Discrete parents&#39; configurations: ## C ## 0 none ## 1 mild ## 2 severe 上記で構築したものが、条件付きガウシアン・ベイジアンネットワーク（CGBN）である。 離散型ノードは多項分布に従う 連続型ノード（親ノードに離散型を含まない）は正規分布に従う 1つ以上の離散型ノードの親を持つ連続型ノードは、離散型変数である親の状態を組み合わせた中における1つの要素の混合正規分布に従う 混合状態における各要素は独立したパラメータを持つ 連続型ノードは連続型ノード、離散型ノードいずれも親にできる 離散型ノードは離散型ノードしか親にできない 4.2 パラメータの推定：混合回帰 すでにDAGは既知であるとする costs &lt;- read.table(&quot;data/healthcare.txt&quot;, header = TRUE, colClasses = c(&quot;factor&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;factor&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;)) #最尤推定法で局所的分布のパラメータを推定した fitted &lt;- bn.fit(dag, data = costs) #今回推定したもの coef(fitted$H) ## A ## H adult old young ## any 0.2380435 0.4239401 0.1060383 ## none 0.7619565 0.5760599 0.8939617 離散型変数 #経験的頻度を用いて推定したもの→つまり実際の値から算出される確率 cpt.H &lt;- prop.table(table(costs[, c(&quot;H&quot;, &quot;A&quot;)]), margin = 2) #これをfitの推定値と比較してみる all.equal(cpt.H, coef(fitted$H)) ## [1] TRUE 同じものが得られている 連続型変数は以下 #lm関数による推定結果と同等となる params.T &lt;- lm(T~I+O, data = costs) all.equal(coef(fitted$T), coef(params.T)) ## [1] TRUE #sigmaについても同様である all.equal(sigma(fitted$T), sigma(params.T)) ## [1] TRUE 離散型の親ノードをもつ連続型のmodelの場合 models.I &lt;- list(lm(I~D, data = costs[costs$C == &quot;none&quot;, ]), lm(I~D, data = costs[costs$C == &quot;mild&quot;, ]), lm(I~D, data = costs[costs$C == &quot;severe&quot;, ])) matrix(c(coef(models.I[[1]]), coef(models.I[[2]]), coef(models.I[[3]])), nrow = 2, ncol = 3, dimnames = list(c(&quot;(Intercept)&quot;, &quot;D&quot;), c(&quot;none&quot;, &quot;mild&quot;, &quot;severe&quot;))) ## none mild severe ## (Intercept) 99.71716 101.3403 117.8585 ## D 299.63817 549.3674 790.6647 c(none = sigma(models.I[[1]]), mild = sigma(models.I[[2]]), severe = sigma(models.I[[3]])) ## none mild severe ## 30.35195 50.23097 94.22670 #以下と同じになる fitted$I ## ## Parameters of node I (conditional Gaussian distribution) ## ## Conditional density: I | C + D ## Coefficients: ## 0 1 2 ## (Intercept) 101.34029 99.71716 117.85851 ## D 549.36741 299.63817 790.66468 ## Standard deviation of the residuals: ## 0 1 2 ## 50.23097 30.35195 94.22670 ## Discrete parents&#39; configurations: ## C ## 0 mild ## 1 none ## 2 severe 既往歴と入院日数、更にはその相互作用を含めたlm関数で算出する結果は異なるので注意 single.model &lt;- lm(I ~ D * C, data = costs) coef(single.model) ## (Intercept) D Cnone Csevere D:Cnone D:Csevere ## 101.340292 549.367410 -1.623135 16.518218 -249.729236 241.297274 sigma(single.model) ## [1] 39.87659 上記のモデルはすべてのデータセットを当てはめてかつ標準誤差は1つしかない 一方でmodels.Iオブジェクトでは既往症(C)の状態ごとに1つの標準誤差が存在する single.modelでは全ての残差が等分散を想定 models.Iでは部分集合ごとで等分散性を想定 4.3 DAG構造の学習：検定とスコア やはりBICを用いる #mixed categorical and normal variablesのときに用いるBICスコア learned &lt;- hc(costs, score = &quot;bic-cg&quot;) modelstring(learned) ## [1] &quot;[A][C|A][H|A][O|A][I|C:H:O][D|C:I][T|I:O]&quot; graphviz.plot(learned) graphviz.plot(dag) #スコアを計算してみたが、Dのせいでマイナス無限大になってしまっていることがわかる print(score(dag, costs, type = &quot;bic-cg&quot;)) ## [1] -Inf print(score(dag, costs, type = &quot;bic-cg&quot;, by.node = TRUE)) ## A C D H I O T ## -2099.902 -1257.209 -Inf -1019.210 -10017.546 -8673.737 -7474.697 これがD|A,H が特異な状態となっている -InfになっているからD→Hの矢印がうまくつながってない 4.4 条件付きガウシアンベイジアンネットワークを使ってみよう 4.4.1 入院日数が少なくとも1日以上(D&gt;=1)で、少なくとも固定費が生じた(I&gt;=100)場合を想定する さらに10^5個のサンプルを得たあとに固定費である100ポンドを除き、入院にかかる支出(I)と入院日数(D)の平均値を関連する分位数とともに計算する part &lt;- cpdist(healthcare, nodes = c(&quot;I&quot;, &quot;D&quot;), evidence = (D &gt;= 1) &amp; (I &gt;= 100), n = 10^5) per.day &lt;- (part$I - 100) / part$D c(mean = mean(per.day), quantile(per.day, c(0.01, 0.99, 0.999))) ## mean 1% 99% 99.9% ## 400.5582 274.9575 814.8299 859.2281 上記より大体想定していた400ポンドに近い値であった（ただし分散は広いが） #通院にかかる費用(O) part &lt;- cpdist(healthcare, nodes = &quot;O&quot;, evidence = (O &gt;= 0), n = 10^5) summary(part$O) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 22.83 65.78 171.63 174.66 204.70 528.37 更に幅が広くなっている 4.4.2 入院日数(D)と既往歴(C)について確認する Dの平均は4.5-5日、Cは40歳で14%なのでyoungではもっと割合は低いし、高齢者なら60%程度は見込まれることが予想される part &lt;- cpdist(healthcare, nodes = &quot;D&quot;, evidence = (H == &quot;any&quot;), n = 10^5) c(mean = mean(part$D), quantile(part$D, c(0.01, 0.99))) ## mean 1% 99% ## 4.5845664 0.2605053 9.7687293 #youngの人で既往歴がmildまたはsevereの人の割合 cpquery(healthcare, event = (C %in% c(&quot;mild&quot;, &quot;severe&quot;)), evidence = (A == &quot;young&quot;), n = 10^5) ## [1] 0.1192491 #oldの人で既往歴がmildまたはsevereの人の割合 cpquery(healthcare, event = (C %in% c(&quot;mild&quot;, &quot;severe&quot;)), evidence = (A == &quot;old&quot;), n = 10^5) ## [1] 0.5755241 ちゃんと高くなっている事がわかる 4.4.3 入院にかかる支出(I)と通院にかかる支出(O)、医療費にかかる税金(T)に関して、シミュレーションを実施 健康管理の費用を賄うことができる税金の額について評価する IもOも0以上であることは前提 正規分布に従う確率変数としてモデル化されているので、負の値も取りうる part &lt;- cpdist(healthcare, nodes = c(&quot;I&quot;, &quot;O&quot;, &quot;T&quot;), evidence = (I &gt;= 0) &amp; (O &gt;= 0), n = 10^5) summary(part$T) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 148.1 313.4 415.6 855.1 626.5 9546.3 finances &lt;- c(mean.tax = mean(part$T), mean.expenditure = mean(part$I + part$O), surplus = mean(part$T) - mean(part$I + part$O)) finances ## mean.tax mean.expenditure surplus ## 855.1486 715.6580 139.4907 print(finances[&quot;mean.tax&quot;] / 12) ## mean.tax ## 71.26239 print(finances[&quot;mean.expenditure&quot;] / 12) ## mean.expenditure ## 59.63817 上記より、ひと月で71.5ポンドが税金として取られて、うち60ポンドが医療費に使われ、残りは再投資or税金免除 4.4.4 では、現状の税金の額で将来健康管理システムを維持できるのか 年齢(A)の分布を変えることでどのように医療費にかかる支出が変化するのか再度シミュレーションを実施する #youngを10%減らし、adultを5%減らし、oldを15%増やした new.A.prob &lt;- array(c(0.30, 0.40, 0.30), dim = 3, dimnames = list(A = A.lv)) new.A.prob ## A ## young adult old ## 0.3 0.4 0.3 healthcare$A &lt;- new.A.prob part &lt;- cpdist(healthcare, nodes = c(&quot;I&quot;, &quot;O&quot;), evidence = (I &gt;= 0) &amp; (O &gt;= 0), n = 10^5) finances[&quot;mean.tax&quot;] - mean(part$I + part$O) ## mean.tax ## 8.957969 もともと余剰金が140ポンドくらいあったのに、わずか10ポンドまで減少した 4.4.5 既往症の保有率が時間とともに変化するかもしれない仮説を検証 重度の既往症を持つ人の割合が微増すれば予算を超えた支出になるはず new.C.prob &lt;- array(c(0.88, 0.10, 0.02, 0.70, 0.22, 0.08, 0.41, 0.51, 0.08), dim = c(3, 3), dimnames = list(C = C.lv, A = A.lv)) #差分を取るとちょっとだけ悪化している人が増えている new.C.prob - C.prob ## A ## C young adult old ## none 0 -0.05 -0.01 ## mild 0 0.02 -0.02 ## severe 0 0.03 0.03 healthcare$C &lt;- new.C.prob part &lt;- cpdist(healthcare, nodes = c(&quot;I&quot;, &quot;O&quot;), evidence = (I &gt;= 0) &amp; (O &gt;= 0), n = 10^5) finances[&quot;mean.tax&quot;] - mean(part$I + part$O) ## mean.tax ## -11.73503 そうするとマイナスになり予算が足りなくなって税金が増加するor治療計画を変更することで入院日数Dを減らすことになる Dを1日減らすことがどれくらい効果的かシミュレーションしてみる #もともとsevere患者の入院日数は7日→6日に変更 new.D.coef &lt;- list(coef = array(c(0, 0, 0, 1, 4, 6), dim = c(1, 6), dimnames = list(&quot;(Intercept)&quot;, NULL)), sd = c(0, 0, 0, 0.5, 1, 1.5)) healthcare$D &lt;- new.D.coef part &lt;- cpdist(healthcare, nodes = c(&quot;I&quot;, &quot;O&quot;), evidence = (I &gt;= 0) &amp; (O &gt;= 0), n = 10^5) finances[&quot;mean.tax&quot;] - mean(part$I + part$O) ## mean.tax ## 45.3292 1日変更するだけで余剰金が40ポンドとなることが予想される "],["時系列データダイナミックベイジアンネットワーク.html", "5 時系列データ：ダイナミック・ベイジアンネットワーク 5.1 事例紹介：ドモティクス(domotics) 5.2 グラフィカル表現 5.3 確率的表現 5.4 DBNの学習 5.5 DBNを使ってみよう 5.6 DBNの図示", " 5 時系列データ：ダイナミック・ベイジアンネットワーク 本章以前のベイジアンネットワークをDBNに拡張する 5.1 事例紹介：ドモティクス(domotics) 民家で屋外の条件に応じて窓を開閉するためのマイクロコントローラのついたパッシブ換気（ただの窓の開け閉める）を設置して空気の質を保つ センサーによって10分ごとに環境状況を測定できる 10分ごとの屋内の空気の質を予測し、屋内の新鮮な空気や快適な温度を保つために窓を開閉することを考える 5.2 グラフィカル表現 項目 略称 説明 風通しの悪さ St 屋内温度 Tin 屋外温度 Tout 窓の開閉 W 二値変数 5.2.1 静的DAG library(bnlearn) #時間情報を取り除いた静的DAG dag &lt;- model2network(&quot;[W][Tout][Tin|W:Tout][St|W]&quot;) graphviz.plot(dag) 5.2.2 ナイーブDAG ある時点での値(t1)はそれに対応した10分前の値(t0)に依存する 静的DAGをコピーして変数t0とt1をリンクさせる 3つの仮定が含まれる ノード間における依存構造はt0とt1で同じ t1はt0に依存しているがその時点より前の変数には依存していない t0とt1はその時点での瞬間ではなく、ある期間にわたっての平均値 限界 t0にかかるノードの周辺分布はt1周辺分布と等しい必要はない パラメータ数が非常に多くなるため、推定時に安定しない t0のノードがt1のノードと同一でt0とt1をリンクしたモデルのほうが望ましい 5.2.3 より倹約的なDAG t0のモデリングは無視してそれらを固定して扱う ノードW1も除外している t0時点の窓の開閉がt1時点のTin/Tout/Stいどのように影響するかの、t0時点のWの効果を検討することが目的であるから 今回はこれを採用する 5.2.4 ロールアップグラフ t0からt1への同一ノードへ向かうアークを示す ループと循環の両方を含むのでDAGとは言えない 5.3 確率的表現 局所分布を扱うことができる これまでの離散型、ガウシアン、CCBNなどをダイナミック・ベイジアンネットワークに適応することが可能 簡単にするため離散型ベイジアンネットワークを用いる T.lv &lt;- c(&quot;&lt;18&quot;, &quot;18-24&quot;, &quot;&gt;24&quot;) Tout0.prob &lt;- array(c(0.20, 0.70, 0.10), dim = 3, dimnames = list(Tout0 = T.lv)) #屋外の気温はそんなに変わらないので80%の確率でt0と同じ Tout1.prob &lt;- array(c(0.80, 0.19, 0.01, 0.10, 0.80, 0.10, 0.01, 0.19, 0.80), dim = c(3, 3), dimnames = list(Tout1 = T.lv, Tout0 = T.lv)) Tout1.prob ## Tout0 ## Tout1 &lt;18 18-24 &gt;24 ## &lt;18 0.80 0.1 0.01 ## 18-24 0.19 0.8 0.19 ## &gt;24 0.01 0.1 0.80 #窓の開閉に関しては確率は0.5と言える W.lv &lt;- c(&quot;open&quot;, &quot;closed&quot;) W0.prob &lt;- array(c(0.5, 0.5), dim = 2, dimnames = list(W0 = W.lv)) W0.prob ## W0 ## open closed ## 0.5 0.5 #風通しの悪さはlow(風通しが良い)とhigh(悪い)のみとしSt0に一様分布を適用することとした St.lv &lt;- c(&quot;low&quot;, &quot;high&quot;) #最初は確率は半分ずつ St0.prob &lt;- array(c(0.50, 0.50), dim = 2, dimnames = list(St0 = St.lv)) #窓が開いているときは風通しが良い(low)６６６６６６６６６６確率が高くなるように設定 St1.prob &lt;- array(c(0.90, 0.10, 0.70, 0.30, 0.70, 0.30, 0.10, 0.90), dim = c(2, 2, 2), dimnames = list(St1 = St.lv, St0 = St.lv, W0 = W.lv)) St1.prob ## , , W0 = open ## ## St0 ## St1 low high ## low 0.9 0.7 ## high 0.1 0.3 ## ## , , W0 = closed ## ## St0 ## St1 low high ## low 0.7 0.1 ## high 0.3 0.9 #快適な温度である確率が高くなるように設定 Tin0.prob &lt;- array(c(0.10, 0.85, 0.05), dim = 3, dimnames = list(Tin0 = T.lv)) #Tin0と同程度の温度となる確率が高くなるように設定 #Toutが高い、低いという値に依存して変化するように設定 #窓の開閉でTin1が変動する確率を変化 #&lt;18から&gt;24へ突然いかないようにした Tin1.prob &lt;- array(c( # W0 = &quot;open&quot;, Tin0 = &quot;&lt;18&quot; 0.875, 0.125, 0, 0.075, 0.9, 0.025, 0.075, 0.7, 0.225, # W0 = &quot;closed&quot;, Tin0 = &quot;&lt;18&quot; 0.875, 0.125, 0, 0.475, 0.5, 0.025, 0.025, 0.65, 0.325, # W0 = &quot;open&quot;, Tin0 = &quot;18-24&quot; 0.475, 0.525, 0, 0.075, 0.8, 0.125, 0, 0.875, 0.125, # W0 = &quot;closed&quot;, Tin0 = &quot;18-24&quot; 0.075, 0.9, 0.025, 0, 0.875, 0.125, 0, 0.475, 0.525, # W0 = &quot;open&quot;, Tin0 = &quot;&gt;24&quot; 0.15, 0.725, 0.125, 0, 0.475, 0.525, 0, 0.475, 0.525, # W0 = &quot;closed&quot;, Tin0 = &quot;&gt;24&quot; 0, 0.125, 0.875, 0, 0.075, 0.925, 0, 0.175, 0.825), dim = c(3, 3, 2, 3), dimnames = list(Tin1 = T.lv, Tout1 = T.lv, W0 = W.lv, Tin0 = T.lv)) Tin1.prob ## , , W0 = open, Tin0 = &lt;18 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.875 0.075 0.075 ## 18-24 0.125 0.900 0.700 ## &gt;24 0.000 0.025 0.225 ## ## , , W0 = closed, Tin0 = &lt;18 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.875 0.475 0.025 ## 18-24 0.125 0.500 0.650 ## &gt;24 0.000 0.025 0.325 ## ## , , W0 = open, Tin0 = 18-24 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.475 0.075 0.000 ## 18-24 0.525 0.800 0.875 ## &gt;24 0.000 0.125 0.125 ## ## , , W0 = closed, Tin0 = 18-24 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.075 0.000 0.000 ## 18-24 0.900 0.875 0.475 ## &gt;24 0.025 0.125 0.525 ## ## , , W0 = open, Tin0 = &gt;24 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.150 0.000 0.000 ## 18-24 0.725 0.475 0.475 ## &gt;24 0.125 0.525 0.525 ## ## , , W0 = closed, Tin0 = &gt;24 ## ## Tout1 ## Tin1 &lt;18 18-24 &gt;24 ## &lt;18 0.000 0.000 0.000 ## 18-24 0.125 0.075 0.175 ## &gt;24 0.875 0.925 0.825 ダイナミック・ベイジアンネットワークを作成する dag &lt;- model2network(paste0(&quot;[W0][St0][Tout0][Tin0][St1|St0:W0]&quot;, &quot;[Tout1|Tout0][Tin1|Tin0:W0:Tout1]&quot;)) graphviz.plot(dag) #確率表を組み合わせる cpt &lt;- list(Tout0 = Tout0.prob, Tout1 = Tout1.prob, W0 = W0.prob, Tin0 = Tin0.prob, Tin1 = Tin1.prob, St0 = St0.prob, St1 = St1.prob) dbn &lt;- custom.fit(dag, cpt) nparams(dbn) ## [1] 52 このようにDBNでは必要となるパラメータが容易に増えてしまう 簡素化を図ったがそれでも多い。 5.4 DBNの学習 #作ったDBNをもとに2000個のサンプルを生成しておく domotics &lt;- rbn(dbn, 2000) write.table(domotics, file = &quot;data/domotics.txt&quot;) DBNに特有の統計手法は特に必要ない 一部異なるところがあるので以下で説明する #t0とt1で別々に学習する必要がある t0.nodes &lt;- c(&quot;W0&quot;, &quot;St0&quot;, &quot;Tout0&quot;, &quot;Tin0&quot;) t1.nodes &lt;- c(&quot;St1&quot;, &quot;Tout1&quot;, &quot;Tin1&quot;) t0でのノードは単独で存在しており、アークはないためブラックリストに入れる bl &lt;- set2blacklist(t0.nodes) t1からt0へ向かうすべてのアークに対してブラックリストに入れる bl &lt;- rbind(bl, tiers2blacklist(list(t0.nodes, t1.nodes))) bl ## from to ## [1,] &quot;St0&quot; &quot;W0&quot; ## [2,] &quot;Tout0&quot; &quot;W0&quot; ## [3,] &quot;Tin0&quot; &quot;W0&quot; ## [4,] &quot;W0&quot; &quot;St0&quot; ## [5,] &quot;Tout0&quot; &quot;St0&quot; ## [6,] &quot;Tin0&quot; &quot;St0&quot; ## [7,] &quot;W0&quot; &quot;Tout0&quot; ## [8,] &quot;St0&quot; &quot;Tout0&quot; ## [9,] &quot;Tin0&quot; &quot;Tout0&quot; ## [10,] &quot;W0&quot; &quot;Tin0&quot; ## [11,] &quot;St0&quot; &quot;Tin0&quot; ## [12,] &quot;Tout0&quot; &quot;Tin0&quot; ## [13,] &quot;St1&quot; &quot;W0&quot; ## [14,] &quot;Tout1&quot; &quot;W0&quot; ## [15,] &quot;Tin1&quot; &quot;W0&quot; ## [16,] &quot;St1&quot; &quot;St0&quot; ## [17,] &quot;Tout1&quot; &quot;St0&quot; ## [18,] &quot;Tin1&quot; &quot;St0&quot; ## [19,] &quot;St1&quot; &quot;Tout0&quot; ## [20,] &quot;Tout1&quot; &quot;Tout0&quot; ## [21,] &quot;Tin1&quot; &quot;Tout0&quot; ## [22,] &quot;St1&quot; &quot;Tin0&quot; ## [23,] &quot;Tout1&quot; &quot;Tin0&quot; ## [24,] &quot;Tin1&quot; &quot;Tin0&quot; #2000個の観測値から予測してみる dbn.hc &lt;- hc(domotics, blacklist = bl) all.equal(dag, dbn.hc) ## [1] TRUE graphviz.plot(dbn.hc) #パラメータ推定しておく dbn.fit &lt;- bn.fit(dbn.hc, domotics) 5.5 DBNを使ってみよう 予測のために用いる点に注意 5.5.1 屋内適温、屋外寒冷、風通しは良くないとき、風通しがよく適温にするために窓を開けるべきか？ #窓を閉めたままパターン print(paste(&quot;close:&quot;, cpquery(dbn.fit, event = (St1 == &quot;low&quot;) &amp; (Tin1 == &quot;18-24&quot;), evidence = (St0 == &quot;high&quot;) &amp; (Tin0 == &quot;18-24&quot;) &amp; (Tout0 == &quot;&lt;18&quot;) &amp; (W0 == &quot;closed&quot;)) ) ) ## [1] &quot;close: 0.0977272727272727&quot; #窓を開けたパターン print(paste(&quot;open:&quot;, cpquery(dbn.fit, event = (St1 == &quot;low&quot;) &amp; (Tin1 == &quot;18-24&quot;), evidence = (St0 == &quot;high&quot;) &amp; (Tin0 == &quot;18-24&quot;) &amp; (Tout0 == &quot;&lt;18&quot;) &amp; (W0 == &quot;open&quot;)) ) ) ## [1] &quot;open: 0.491189427312775&quot; 以上より窓を開けることでより達成に近づくがその確率は0.5を下回っており低い 5.5.2 t0で屋外の状態が不明なとき、窓を開けるべきか cpquery(dbn.fit, event = (St1 == &quot;low&quot;) &amp; (Tin1 == &quot;18-24&quot;), evidence = (St0 == &quot;high&quot;) &amp; (Tin0 == &quot;18-24&quot;) &amp; (W0 == &quot;closed&quot;)) ## [1] 0.09401709 cpquery(dbn.fit, event = (St1 == &quot;low&quot;) &amp; (Tin1 == &quot;18-24&quot;), evidence = (St0 == &quot;high&quot;) &amp; (Tin0 == &quot;18-24&quot;) &amp; (W0 == &quot;open&quot;)) ## [1] 0.5702792 この結果はTout0における平均的な状態で条件づけた場合の確率を示す 5.5.3 このプロセスを自動化する evidence &lt;- data.frame(St0 = factor(&quot;high&quot;, levels = St.lv), Tin0 = factor(&quot;18-24&quot;, levels = T.lv), Tout0 = factor(&quot;&lt;18&quot;, levels = T.lv), W0 = factor(&quot;open&quot;, levels = W.lv)) t0において観察される状態を生成しておく #まずTin1を予測する predict(dbn.fit, data = evidence, node = &quot;Tin1&quot;, method = &quot;bayes-lw&quot;) ## [1] 18-24 ## Levels: &lt;18 18-24 &gt;24 #Tin1をもとに続いてSt1の状態を予測する evidence$Tin1 &lt;- factor(&quot;18-24&quot;, levels = T.lv) predict(dbn.fit, data = evidence, node = &quot;St1&quot;, method = &quot;bayes-lw&quot;) ## [1] low ## Levels: low high lowとなったことから、openにすることで適温かつ風通しが良い状態になることが予測される #逆にclosedにするとhighとなり風通しが悪くなることが予想される evidence &lt;- data.frame(St0 = factor(&quot;high&quot;, levels = St.lv), Tin0 = factor(&quot;18-24&quot;, levels = T.lv), Tout0 = factor(&quot;&lt;18&quot;, levels = T.lv), W0 = factor(&quot;closed&quot;, levels = W.lv)) predict(dbn.fit, data = evidence, node = &quot;Tin1&quot;, method = &quot;bayes-lw&quot;) ## [1] 18-24 ## Levels: &lt;18 18-24 &gt;24 evidence$Tin1 &lt;- factor(&quot;18-24&quot;, levels = T.lv) predict(dbn.fit, data = evidence, node = &quot;St1&quot;, method = &quot;bayes-lw&quot;) ## [1] high ## Levels: low high 5.6 DBNの図示 DAGは左から右に流れるように表示する library(Rgraphviz) #一旦グラフオブジェクトを作成する gR &lt;- graphviz.plot(dag) #t0とt1でグルーピングする t0.nodes &lt;- c(&quot;W0&quot;, &quot;St0&quot;, &quot;Tout0&quot;, &quot;Tin0&quot;) t1.nodes &lt;- c(&quot;St1&quot;, &quot;Tout1&quot;, &quot;Tin1&quot;) #sg0 &lt;- list(graph = subGraph(t0.nodes, gR), cluster = TRUE) #sg1 &lt;- list(graph = subGraph(t1.nodes, gR), cluster = TRUE) ここで各ノードをグループ化していい感じに表示できるはずだが、subGraphの関数のエラー？により描画できず。。 分かる方教えてほしいです。 参考文献はこちら "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
